{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <p style=\"text-align: center;\">MIS 382: Advanced Predictive Modeling</p>\n",
    "# <p style=\"text-align: center;\">Assignment 3</p>\n",
    "## <p style=\"text-align: center;\">Total points: 60</p>\n",
    "## <p style=\"text-align: center;\">Due: Wednesday, October 25th, submitted via Canvas by 11:59 pm</p>\n",
    "### <p style=\"text-align: center;\">Meyappan Subbaiah - ms47296 and Matthew Barrett - mb58428\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UTID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group.  \n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - Gradient Descent (5+8+2=15pts)\n",
    "\n",
    "In this question you will implement vanilla SGD and an adaptive gradient update technique called Adagrad. In addition, you will also implement ridge regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using stochastic gradient descent, derive the coefficent updates for all 4 coefficients of the model: $$ y = w_0 + w_1*x_1 + w_2*x_1*x_2 + w_3*x_2 $$ Hint: start from the cost function (Assume sum of squared error). If you write the math by hand, submit that as a separate file and make a reference to it in your notebook or include the image in your notebook.\n",
    "2. Write Python code for an SGD solution to the non-linear model $$ y = w_0 + w_1*x_1 + w_2*x_1*x_2 + w_3*x_2$$ Try to format similarly to scikit-learn's models. Your class should take as input the learning_rate, regularization_constant and number of epochs. The fit method must take as input X,y and a choice of update_rule as 'SGD' or 'adagrad' (Notes on implementation below). The _predict_ method takes an X value (optionally, an array of values). Use your new gradient descent regression to predict the data given in 'samples.csv', for 10 epochs, using learning rates: [.0001, .001, .01, 0.1, 1, 10, 100] and regularization constants in the range: [0,10,100] . Plot MSE and the $w$ parameters as a function of epoch count (10 epochs) for the best 2 combinations of learning_rate and regularization for both SGD and Adagrad. ie you should have 2 plots of MSE and parameter updates for SGD and adagrad each. Report the MSE at the end of 10 epochs for all 4 combinations.\n",
    "3. Based on the experiments, answer the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which of the 2 techniqes allows for larger initial setting of the learning_rate? Why?\n",
    "2. What would a drawback of adagrad be? How could this be fixed? (Hint: Adadelta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "#### Formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights are updated using the following formula:\n",
    "$$w_{j} = w_{j} - \\eta\\Delta w_{j} $$\n",
    "\n",
    "In this case $\\Delta w_{j} = \\dfrac {\\delta E_{n}}{\\delta w_{j}}$, where $E_{n}$ is our cost function.\n",
    "\n",
    "The cost function is defined as: $E_{n} = (t_{n}-y_{n})^2 + \\lambda * ||w||^2 $, where y is given in the problem statement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient update $\\Delta w_{j}$ is found for each coefficient from \n",
    "\n",
    "$$\\frac {\\delta E_{n}}{\\delta w_{0}} = -2 (t_{n} - y_{n}) - 2 * \\lambda *w_0  $$\n",
    "\n",
    "$$\\frac {\\delta E_{n}}{\\delta w_{1}} = -2*x_{1} (t_{n} - y_{n}) - 2 * \\lambda *w_1 $$\n",
    "\n",
    "$$\\frac {\\delta E_{n}}{\\delta w_{2}} = -2*x_{1}*x_{2} (t_{n} - y_{n}) - 2 * \\lambda *w_2 $$\n",
    "\n",
    "$$\\frac {\\delta E_{n}}{\\delta w_{3}} = -2*x_{2} (t_{n} - y_{n}) - 2 * \\lambda *w_3 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Coefficient updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, our final weight updates are:\n",
    "\n",
    "$$ w_{0} = w_{0} + 2\\eta* (t_{n} - y_{n}) $$\n",
    "$$ w_{1} = w_{1} + 2\\eta * x_{1}(t_{n} - y_{n})$$\n",
    "$$ w_{2} = w_{2} + 2\\eta * x_{1}*x_{2}(t_{n} - y_{n}) $$\n",
    "$$ w_{3} = w_{3} + 2\\eta * x_{2}(t_{n} - y_{n}) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StochasticGradient():\n",
    "    \n",
    "    def __init__(self, learnrate, reg_constant, epoch):\n",
    "        \"\"\"Accept basic inputs for the SG class:\n",
    "           (1) learning rate (2) reg_constant and (3) epochs\"\"\"\n",
    "        self.learnrate = learnrate\n",
    "        self.reg_constant = reg_constant\n",
    "        self.epoch = epoch\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Accept X and Y data\"\"\"\n",
    "        X = normalize(X)\n",
    "        train, tgt = shuffle(X,y)\n",
    "        ## Define Weights \n",
    "        weights = np.array([.0005]*4)\n",
    "        w0 = weights[0]\n",
    "        w1 = weights[1]\n",
    "        w2 = weights[2]\n",
    "        w3 = weights[3]\n",
    "        \n",
    "        for ep in range(self.epoch):\n",
    "            for reg in range(self.reg_constant):\n",
    "                x1 = train[ep][0]\n",
    "                x2 = train[ep][1]\n",
    "                eta = self.learnrate\n",
    "                t = pred[ep]\n",
    "                y = w0 + w1*x1 + w2*x1*x2 + w3*x2\n",
    "                \n",
    "                # weight updates\n",
    "                w0 = w0 + 2*(t-y)*eta + 2*ep*w0\n",
    "                w1 = w1 + 2*x1*(t-y)*eta + 2*ep*w1\n",
    "                w2 = w2 + 2*x1*x2*(t-y)*eta + 2*ep*w2\n",
    "                w3 = w3 + 2*x2*(t-y)*eta + 2*ep*w3\n",
    "                \n",
    "                nweights = [w0, w1, w2, w3]\n",
    "            ## Reshuffle data\n",
    "            train, y = shuffle(X,y) \n",
    "        ## Save weights to use in predict\n",
    "        self.weights = weights\n",
    "        \n",
    "    def predict(self, X, weights = None):\n",
    "        \n",
    "        pred = []\n",
    "        \n",
    "        ## Set original weights\n",
    "        if weights == None:\n",
    "            w0 = self.weights[0]\n",
    "            w1 = self.weights[1]\n",
    "            w2 = self.weights[2]\n",
    "            w3 = self.weights[3]\n",
    "        ## weight updates    \n",
    "        else:\n",
    "            w0 = weights[0]\n",
    "            w1 = weights[1]\n",
    "            w2 = weights[2]\n",
    "            w3 = weights[3]\n",
    "            \n",
    "        n = self.learnrate\n",
    "        \n",
    "        for p in X:\n",
    "            x1 = p[0]\n",
    "            x2 = p[1]\n",
    "            \n",
    "            y_pred = w0 + w1*x1 + w2*x1*x2 + w3*x2\n",
    "            \n",
    "            pred.append(y_pred)\n",
    "            \n",
    "        self.pred = pred                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = pd.read_csv('samples.csv', names=['index', 'x1', 'x2', 'target'], header = 0)\n",
    "features = samples[['x1', 'x2']]\n",
    "target = samples['target']\n",
    "\n",
    "X, y = np.array(features), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD learning rate 0.0001 Reg Constant 0 MSE 17.3376541822 \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Singleton array -0.00032017800532322738 cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-e8d8c30b3eee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreg_constant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStochasticGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearnrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_constant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0msg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-3b714e72a597>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mnweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m## Reshuffle data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m## Save weights to use in predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/sklearn/utils/__init__.pyc\u001b[0m in \u001b[0;36mshuffle\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \"\"\"\n\u001b[1;32m    284\u001b[0m     \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/sklearn/utils/__init__.pyc\u001b[0m in \u001b[0;36mresample\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m    202\u001b[0m                                                     n_samples))\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \"\"\"\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0;32m--> 126\u001b[0;31m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array -0.00032017800532322738 cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "learnrate = [.0001, .001, .01, 0.1, 1, 10, 100]\n",
    "reg_constant = [0,10,100]\n",
    "\n",
    "for learn in learnrate:\n",
    "    for reg in reg_constant:\n",
    "        sg1 = StochasticGradient(learnrate = learn, reg_constant = reg, epoch = 5)\n",
    "        sg1.fit(X,y)\n",
    "        sg1.predict(X)\n",
    "        pred = sg1.pred\n",
    "        mse_val = mean_squared_error(y,pred)\n",
    "        print(\"SGD learning rate %s Reg Constant %s MSE %s \"% (lr,reg,mse_val)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Adagrad\n",
    "Adagrad (http://ruder.io/optimizing-gradient-descent/) differs from vanilla SGD in that the learning rate of each weight changes over updates. A cache is maintained that holds the sum of squares of all gradients upto the current update. The learning_rate is divided by the cache, resulting in a different learning rate for each weight. A consequence of this update rule is that weights that have already seen large gradients (made large jumps) make smaller updates in subsequent iterations.\n",
    "Specifically, the steps can be listed as below:\n",
    "1. cache = cache + (gradients^2)\n",
    "2. weights = weights + ((learning_rate)/sqrt(cache+1e-6))*gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Notes on Training with Gradient Descent\n",
    "1. Compute error: This consists of a prediction error and a regularization term. From an implementation perspective, this is a function that takes as input the truth, prediction and regularization hyperparameter and returns an error\n",
    "2. Compute gradients: Take a derivative of the error in terms of the weights. This can be modelled as a function that takes as input the error and features and returns the gradients for each weight\n",
    "3. Update weights: Weight updates can be done using vanilla SGD or adaptive techniques. The update function takes as inputs the gradient and hyperparameters and returns the new weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. (6+4=10 pts) Tensor Playground\n",
    "Visit http://playground.tensorflow.org for this problem\n",
    "\n",
    "A. From the far right, select \"Regression\" as the problem type, and select the 2nd of the two data sets ( the right one ).  \n",
    "\n",
    "   i) What sort of test / training loss do you get if you run it for 200 epochs with the following learning rates: .3, .01 and .003 ?  What if you run it for 1000 epochs with these learning rates?  Leave all other values at their defaults ( test/training ratio 50%, Noise 0, Batch Size 10, using Tanh activation function, and No Regularization )\n",
    "   \n",
    "   ii) Keeping learning rate at .3, Activation at Tanh, with all others at their defaults, and running for 200 epochs.  \n",
    "     What sort of test/train loss can you achieve using only 1 neuron in the first hidden layer.  What about for 2,3 or 8 neurons?  Provide screen shots of output layer and comment on how the different output fits look and compare with one another.\n",
    "\n",
    "   iii)Now keeping learning rate at .03 with all others at their defaults, and running for 200 epochs.  \n",
    "       Compare how the activation functions affect the ouput ( ReLU, Sigmoid, Tanh, Linear ). Provide screen shots of output results and comment.\n",
    "\n",
    "\n",
    "B. Neural Nets can fit anything.  Now reset to the initial defaults, and select \"Classification\" as the problem type, and from the Data section, select the bottom right \"Spriral\" data set.  With the idea of trying to minimize training/testing error, provide solutions to the problem for the following 2 scenarios.  i) Using just the first 2 inputs ( as per default ) and ii) Using all 7 of the inputs.  You may use as many layers as you want, whatever activation, however man neurons.  Provide screen shots which show your full network, output and parameters. Briefly justify your decisions, and comment on difficulties/tradeoffs, what helps/what doesn't,etc. \n",
    "\n",
    "## ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A) Tensor Flow Run 1: 200 epochs, learning 0.3  \n",
    "(B) Tensor Flow Run 2: 200 epochs, learning 0.01  \n",
    "(C) Tensor Flow Run 3: 200 epochs, learning 0.003  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A | B | C\n",
    "- | - | -\n",
    "![alt](epoch_200_lr_0.3.png) | ![alt](epoch_200_lr_0.01.png) | ![alt](epoch_200_lr_0.003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(D) Tensor Flow Run 1: 1000 epochs, learning 0.3  \n",
    "(E) Tensor Flow Run 2: 1000 epochs, learning 0.01  \n",
    "(F) Tensor Flow Run 3: 1000 epochs, learning 0.003  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D | E | F\n",
    "- | - | -\n",
    "![alt](epoch_1000_lr_0.3.png) | ![alt](epoch_1000_lr_0.01.png) | ![alt](epoch_1000_lr_0.003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** [Insert discussion here for the tensor flow of 200 epochs vs 1000 epochs) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A) 200 Epochs, 1 Neurons  \n",
    "(B) 200 Epochs, 2 Neurons  \n",
    "(C) 200 Epochs, 3 Neurons  \n",
    "(D) 200 Epochs, 8 Neurons  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A | B | C | D\n",
    "- | - | - | -\n",
    "![alt](epoch_200_1_neurons.png) | ![alt](epoch_200_2_neurons.png) | ![alt](epoch_200_3_neurons.png) | ![alt](epoch_200_8_neurons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** [Insert discussion here for the tensor flow of 200 epochs vs 1000 epochs) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A) Tanh  \n",
    "(B) Sigmoid  \n",
    "(C) Relu  \n",
    "(D) Linear  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A | B | C | D\n",
    "- | - | - | -\n",
    "![alt](epoch_200_tanh.png) | ![alt](epoch_200_sigmoid.png) | ![alt](epoch_200_relu.png) | ![alt](epoch_200_linear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Neural Nets can fit anything. Now reset to the initial defaults, and select \"Classification\" as the problem type, and from the Data section, select the bottom right \"Spriral\" data set. With the idea of trying to minimize training/testing error, provide solutions to the problem for the following 2 scenarios. i) Using just the first 2 inputs ( as per default ) and ii) Using all 7 of the inputs. You may use as many layers as you want, whatever activation, however man neurons. Provide screen shots which show your full network, output and parameters. Briefly justify your decisions, and comment on difficulties/tradeoffs, what helps/what doesn't,etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B\n",
    "#### Part 1 - First 2 inputs (default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](epoch_part_b_2_inputs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** [Insert Discussions here] **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 - All 7 inputs (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "More Neurons (1) Costs: Comp time, (2) overfitting is a problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](epoch_part_b_7_inputs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Multi-layer Perceptron Regressor (15 points)\n",
    "\n",
    "In this question, you will explore the application of Multi-layer Perceptron (MLP) regression using sklearn package in Python. We will use the Appliances energy prediction dataset for this problem https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction.\n",
    "\n",
    "Following code will pre-process the data and split the data into training and test set using [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) with **random state 42** and **test_size = 0.33**.\n",
    "Make sure you have 56 variables and one target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((13222, 56), (13222,), (6513, 56), (6513,))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,KFold)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "data = pd.read_csv('./energydata_complete.csv') \n",
    "\n",
    "y = data['Appliances']\n",
    "X = data.drop(['date','Appliances', 'rv1', 'rv2'], axis = 1)\n",
    "\n",
    "\n",
    "from dateutil import parser\n",
    "timeData = np.array(data['date'])\n",
    "\n",
    "days = []\n",
    "hours = []\n",
    "for line in xrange(len(timeData)):\n",
    "    day = parser.parse(timeData[line]).weekday()\n",
    "    hour = parser.parse(timeData[line]).hour\n",
    "    days.append(day)\n",
    "    hours.append(hour)\n",
    "    \n",
    "X = pd.concat([X, pd.get_dummies(days), pd.get_dummies(hours)], axis = 1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.33, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to use in this problem is [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Instead of fitting a model on original data, use StandardScaler to make each feature centered ([Example](http://scikit-learn.org/stable/auto_examples/applications/plot_prediction_latency.html#sphx-glr-auto-examples-applications-plot-prediction-latency-py)). Whenever you have training and test data, fit a scaler on training data and use this scaler on test data. Here, scale only features (independent variables), not target variable y.\n",
    "\n",
    "Use [sklearn.neural_nework.MLPRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor) to do a 5-fold cross validation using sklearn's [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold). The cross validation must be performed on the **training data**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Use following parameter settings for MLPRegressor:\n",
    "\n",
    "    activation = 'tanh', solver = 'sgd', learning_rate='constant', random_state=42,\n",
    "    batch_size=5000, learning_rate_init = 0.005\n",
    "    \n",
    "Now, consider three different settings for the number of hidden units:\n",
    "    \n",
    "   (a) *hidden_layer_sizes = (2,)* (b) *hidden_layer_sizes = (30,10)* (c) *hidden_layer_sizes = (100,)*\n",
    "    \n",
    "   Report the average Root Mean Squared Error (RMSE) value based on your 5-fold cross validation for each model: (a), (b), and (c) (5pts)\n",
    "      \n",
    "2) Now, using the same number of hidden units used in part 1), train MLPRegressor models on whole training data and report RMSE score for both Train and Test set (Again, use StandardScaler). Which model works the best, (a), (b), or (c)? Briefly analyze the result in terms of the number of hidden units. (3pts)\n",
    "\n",
    "3) MLPRegressor has a built-in attribute *loss\\_curve\\_* which returns the loss at each epoch (misleadingly referred to as \"iteration\" in scikit documentation, though they use epoch in the actual code!). For example, if your model is named as *my_model* you can call it as *my\\_model.loss\\_curve\\_* ([example](http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py)). Plot three curves for model (a), (b), and (c) in one figure, where *X-axis* is epoch  number and *Y-axis* is squared root of *loss\\_curve\\_* value. (2pts)\n",
    "\n",
    "4) Use following parameter settings for MLPRegressor:\n",
    "\n",
    "    activation = 'tanh', solver = 'sgd', learning_rate='constant', random_state=42,\n",
    "    batch_size=5000, hidden_layer_sizes = (15,)\n",
    "    \n",
    "Now, consider three different settings for the learning rates:\n",
    "    \n",
    "   (i) *learning_rate_init = 0.005* (ii) *learning_rate_init = 0.01* (iii) *learning_rate_init = 1*\n",
    "    \n",
    "   Report the average Root Mean Squared Error (RMSE) value based on your 5-fold cross validation for each model: (i), (ii), and (iii) (5pts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer\n",
    "#### Part 1 - Training Set, 5 Fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler()\n",
    "\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "\n",
    "X_test = X_scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models (MLP-Regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp2 = MLPRegressor(hidden_layer_sizes=(2,), activation='tanh', solver='sgd', learning_rate='constant', random_state=42, \n",
    "                   batch_size=5000, learning_rate_init=0.005)\n",
    "\n",
    "\n",
    "mlp30 = MLPRegressor(hidden_layer_sizes=(30,10), activation='tanh', solver='sgd', learning_rate='constant', random_state=42,\n",
    "                    batch_size=5000, learning_rate_init=0.005)\n",
    "\n",
    "mlp100 = MLPRegressor(hidden_layer_sizes=(100,), activation='tanh', solver='sgd', learning_rate='constant', random_state=42,\n",
    "                    batch_size=5000, learning_rate_init=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = KFold(n_splits=5, random_state=42)\n",
    "cv.get_n_splits(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmse_2 = []\n",
    "rmse_30 = []\n",
    "rmse_100 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for train, test in cv.split(X_train):\n",
    "    # MLP Fits\n",
    "    mlp2.fit(X_train.iloc[train], y_train.iloc[train])\n",
    "    mlp30.fit(X_train.iloc[train], y_train.iloc[train])\n",
    "    mlp100.fit(X_train.iloc[train], y_train.iloc[train])\n",
    "    \n",
    "    # compare predictions against y-true value \n",
    "    y_true = y_train.iloc[test]\n",
    "    # mlp 2 predictions\n",
    "    y2_predict = mlp2.predict(X_train.iloc[test])\n",
    "    # mlp 30 predictions\n",
    "    y30_predict = mlp30.predict(X_train.iloc[test])\n",
    "    # mlp 100 predictions\n",
    "    y100_predict = mlp100.predict(X_train.iloc[test])\n",
    "    \n",
    "    mse_2 = mean_squared_error(y_true, y2_predict)\n",
    "    mse_30 = mean_squared_error(y_true, y30_predict)\n",
    "    mse_100 = mean_squared_error(y_true, y100_predict)\n",
    "    \n",
    "    rmse_2.append(mse_2**.5)\n",
    "    rmse_30.append(mse_30**.5)\n",
    "    rmse_100.append(mse_100**.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The average RMSE values reported below were found on the training sets with a 5 fold cross-validaition.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Hidden Layers, Average RMSE: 95.00\n",
      "30 Hidden Layers, Average RMSE: 102.59\n",
      "100 Hidden Layers, Average RMSE: 81.65\n"
     ]
    }
   ],
   "source": [
    "print \"2 Hidden Layers, Average RMSE: {:.2f}\".format(np.mean(rmse_2))\n",
    "print \"30,10 Hidden Layers, Average RMSE: {:.2f}\".format(np.mean(rmse_30))\n",
    "print \"100 Hidden Layers, Average RMSE: {:.2f}\".format(np.mean(rmse_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 - Full Data \n",
    "##### Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='tanh', alpha=0.0001, batch_size=5000, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.005, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=42, shuffle=True,\n",
       "       solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp2.fit(X_train, y_train)\n",
    "mlp30.fit(X_train, y_train)\n",
    "mlp100.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict2_train = mlp2.predict(X_train)\n",
    "predict2_test = mlp2.predict(X_test)\n",
    "\n",
    "predict30_train = mlp30.predict(X_train)\n",
    "predict30_test = mlp30.predict(X_test)\n",
    "\n",
    "predict100_train = mlp100.predict(X_train)\n",
    "predict100_test = mlp100.predict(X_test)\n",
    "\n",
    "rmse2_train = mean_squared_error(y_train, predict2_train)**.5\n",
    "rmse2_test = mean_squared_error(y_test, predict2_test)**.5\n",
    "\n",
    "rmse30_train = mean_squared_error(y_train, predict30_train)**.5\n",
    "rmse30_test = mean_squared_error(y_test, predict30_test)**.5\n",
    "\n",
    "rmse100_train = mean_squared_error(y_train, predict100_train)**.5\n",
    "rmse100_test = mean_squared_error(y_test, predict100_test)**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The average RMSE values reported below were found on the test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Hidden Layers, Average RMSE: 93.15\n",
      "30 Hidden Layers, Average RMSE: 85.73\n",
      "100 Hidden Layers, Average RMSE: 80.58\n"
     ]
    }
   ],
   "source": [
    "print \"2 Hidden Layers, Average RMSE: {:.2f}\".format(rmse2_test)\n",
    "print \"30,10 Hidden Layers, Average RMSE: {:.2f}\".format(rmse30_test)\n",
    "print \"100 Hidden Layers, Average RMSE: {:.2f}\".format(rmse100_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The average RMSE values reported below were found on the training sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Hidden Layers, Average RMSE: 92.44\n",
      "30 Hidden Layers, Average RMSE: 78.65\n",
      "100 Hidden Layers, Average RMSE: 67.78\n"
     ]
    }
   ],
   "source": [
    "print \"2 Hidden Layers, Average RMSE: {:.2f}\".format(rmse2_train)\n",
    "print \"30,10 Hidden Layers, Average RMSE: {:.2f}\".format(rmse30_train)\n",
    "print \"100 Hidden Layers, Average RMSE: {:.2f}\".format(rmse100_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion: Insert some comments about test vs train sets on the full data set vs 5-fold CV. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3 - Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAEYCAYAAAC+8+djAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1219ef410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Sqrt Loss vs. Epochs\", fontsize=16)\n",
    "plt.ylabel('Square Root of Loss')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.plot(np.sqrt(mlp2.loss_curve_),label='2 Hidden Layers')\n",
    "plt.plot(np.sqrt(mlp30.loss_curve_),color='g',label='30,10 Hidden Layers')\n",
    "plt.plot(np.sqrt(mlp100.loss_curve_),color='r',label= '100 Hidden Layers')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4 - Learning Rates manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Re-create data to ensure previous manipulation doesn't affect it\n",
    "## probably don't need it, but just in-case.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.33, random_state=42)\n",
    "X_scaler = StandardScaler()\n",
    "\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "\n",
    "X_test = X_scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_lr1 = MLPRegressor(hidden_layer_sizes=(15,), activation='tanh', solver='sgd', learning_rate='constant', random_state=42, \n",
    "                   batch_size=5000, learning_rate_init=0.005)\n",
    "\n",
    "\n",
    "mlp_lr2 = MLPRegressor(hidden_layer_sizes=(15,), activation='tanh', solver='sgd', learning_rate='constant', random_state=42,\n",
    "                    batch_size=5000, learning_rate_init=0.01)\n",
    "\n",
    "mlp_lr3 = MLPRegressor(hidden_layer_sizes=(15,), activation='tanh', solver='sgd', learning_rate='constant', random_state=42,\n",
    "                    batch_size=5000, learning_rate_init=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=42)\n",
    "kf.get_n_splits(X_train)\n",
    "rmse_lr1 = []\n",
    "rmse_lr2 = []\n",
    "rmse_lr3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for train, test in kf.split(X_train):\n",
    "    # MLP Fits\n",
    "    mlp_lr1.fit(X_train.iloc[train], y_train.iloc[train])\n",
    "    mlp_lr2.fit(X_train.iloc[train], y_train.iloc[train])\n",
    "    mlp_lr3.fit(X_train.iloc[train], y_train.iloc[train])\n",
    "    \n",
    "    # compare predictions against y-true value \n",
    "    y_true = y_train.iloc[test]\n",
    "    # mlp 2 predictions\n",
    "    y2_predict = mlp_lr1.predict(X_train.iloc[test])\n",
    "    # mlp 30 predictions\n",
    "    y30_predict = mlp_lr2.predict(X_train.iloc[test])\n",
    "    # mlp 100 predictions\n",
    "    y100_predict = mlp_lr3.predict(X_train.iloc[test])\n",
    "    \n",
    "    mse_lr1 = mean_squared_error(y_true, y2_predict)\n",
    "    mse_lr2 = mean_squared_error(y_true, y30_predict)\n",
    "    mse_lr3 = mean_squared_error(y_true, y100_predict)\n",
    "    \n",
    "    rmse_lr1.append(np.sqrt(mse_lr1))\n",
    "    rmse_lr2.append(np.sqrt(mse_lr2))\n",
    "    rmse_lr3.append(np.sqrt(mse_lr3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The average RMSE values reported below were found on the training sets with a 5 fold cross-validaition.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 Hidden Layers, Learning Rate = 0.005, Average RMSE: 88.33\n",
      "15 Hidden Layers, Learning Rate = 0.01 , Average RMSE: 90.28\n",
      "15 Hidden Layers, Learning Rate = 1    , Average RMSE: 51785446645.84\n"
     ]
    }
   ],
   "source": [
    "print \"15 Hidden Layers, Learning Rate = 0.005, Average RMSE: {:.2f}\".format(np.mean(rmse_lr1))\n",
    "print \"15 Hidden Layers, Learning Rate = 0.01 , Average RMSE: {:.2f}\".format(np.mean(rmse_lr2))\n",
    "print \"15 Hidden Layers, Learning Rate = 1    , Average RMSE: {:.2f}\".format(np.mean(rmse_lr3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: (2+5+3=10 pts) Bayes Decision Theory\n",
    "a. Explain what you understand by class-conditional likelihood, class priors, and posterior probability of a class given an input, and the relationship between them. Please define all symbols and equations used explicitly.\n",
    "( 2 points )\n",
    "\n",
    "b. Suppose you want to learn a binary classifier to predict whether or not a customer will buy a TV. The class label is 0 if the customer does not buy a TV and 1 if they do. For each customer, you are given two features, $x_1$ is the per hour salary and $x_2$ is the age. Assume that the class conditional distribution $p([x_1 , x_2]|C)$ is Gaussian. The mean salary and age of the people who do buy a TV is 15 and 30 respectively and that of those who dont is 8.5 and 25. Assume that both classes of customers have the same covariance given by the Identity matrix. Further, your sales data suggests that only 1 in 3 people actually bought a TV in the last few years. Mathematically derive the (optimal) Bayes decision boundary for this problem. (5 points)\n",
    "\n",
    "c. Now write code to sample 100 customers from each class (C = 0, 1) under the assumed distribution and the estimated parameters and plot their features. Additionally, plot the decision boundary you obtained in the part (b) on the same plot. (3 points)\n",
    "\n",
    "## ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Decision Tree using Python (10 pts)\n",
    "In this problem, you will model the data using decision trees to perform a classification task. Load the BreastTissue.csv dataset. The dataset has been preprocessed. The description of the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Breast+Tissue). Using the class tree.DecisionTreeClassifier (http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree), build two different trees with a maximum depth of two using the split criteria (i) Gini and (ii) Entropy. Use all the data to build trees. Plot the two trees. If your classifier object is called clf, use the following commands to save the generated tree as a '.dot' file that can be used to visualize the tree using Webgraphviz: http://www.webgraphviz.com/\n",
    "\n",
    "Hint: see  http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn import tree\n",
    "from sklearn.externals.six import StringIO\n",
    "with open(\"decision_tree_gini.dot\", 'w') as f:\n",
    "    f = tree.export_graphviz(clf, out_file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy paste the contents of your '.dot' file into the text window on the website to visualize the trees. At which node(s) do they differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.externals.six import StringIO\n",
    "data = pd.read_csv('BreastTissue.csv')\n",
    "Y = data['Class']\n",
    "X = data.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model (Gini, Max Depth = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion='gini',max_depth=2)\n",
    "clf.fit(X,Y)\n",
    "with open(\"decision_tree_gini.dot\", 'w') as f:\n",
    "    f = tree.export_graphviz(clf, out_file=f,class_names=['adi','car', 'mas', 'gla', 'fad', 'con'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Gini_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model (Entropy, Max Depth = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion='entropy',max_depth=2)\n",
    "clf.fit(X,Y)\n",
    "with open(\"decision_tree_entropy.dot\", 'w') as f:\n",
    "    f = tree.export_graphviz(clf, out_file=f,class_names=['adi','car', 'mas', 'gla', 'fad', 'con'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Entropy_Plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSERT DISCUSSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
