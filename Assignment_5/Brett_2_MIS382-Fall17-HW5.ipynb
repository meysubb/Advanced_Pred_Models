{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <p style=\"text-align: center;\">MIS382: Advanced Predictive Modeling</p>\n",
    "# <p style=\"text-align: center;\">Assignment 5</p>\n",
    "## <p style=\"text-align: center;\">Total points: 35</p>\n",
    "## <p style=\"text-align: center;\">Due: Monday, November 27th, submitted via Canvas by 11:59 pm</p>\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UTID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group.  \n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Ensembles (1+12+2 = 15pts)\n",
    "In this question, we will compare performance of different ensemble methods: [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), [Gradient Boosting Decision Tree](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) (GBDT), and [XGBoost](http://xgboost.readthedocs.io/en/latest/).  Note that you have to install xgboost package in addition to scikit-learn.  You can see installation guides [here](http://xgboost.readthedocs.io/en/latest/build.html).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Two  datasets are provided for this problem. For **each of the datasets ((X1.csv, y1.csv), (X2.csv, y2.csv))**, do the following:\n",
    "\n",
    "1. Load the data and partition it into features (X) and the target label (y) for classification task. Then, use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split data into training and testing: test_size=0.33, random_state=42.\n",
    "\n",
    "2. Build a classifier using [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), [Gradient Boosting Decision Tree](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html), and [XGBoost](http://xgboost.readthedocs.io/en/latest/), respectively, and answer the following for each classifier.\n",
    "\n",
    " - Mention any design choices (with reasoning/justification) that you made, e.g. the hyperparameters considered for each classifier.\n",
    " - Report the mean error rate (fraction of incorrect labels) and the confusion matrix on test data. <br>\n",
    " - Report the feature importance and time of execution (training and predicting times).\n",
    "\n",
    "3. Compare the three classifiers for the two different datasets ((X1.csv, y1.csv), (X2.csv, y2.csv)) in terms of the misclassification rate.  What are the characteristics of the dataset and the classifiers that resulted in somewhat different comparative results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from xgboost.sklearn import XGBClassifier\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = pd.read_csv('X1.csv', header = None)\n",
    "x2 = pd.read_csv('X2.csv', header = None)\n",
    "\n",
    "y1 = pd.read_csv('y1.csv', header = None)\n",
    "y1 = y1[0].values\n",
    "y2 = pd.read_csv('y2.csv', header = None)\n",
    "y2 = y2[0].values\n",
    "\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42)\n",
    "\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X1 and Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 25, 50], 'criterion': ['gini', 'entropy']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFC = RandomForestClassifier()\n",
    "param_grid_RFC = { 'n_estimators' : [5,25,50],\n",
    "                  'criterion' : ['gini', 'entropy']}\n",
    "cv_RFC = GridSearchCV(estimator = RFC, param_grid=param_grid_RFC, cv=5)\n",
    "cv_RFC.fit(x1_train, y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 25, 'criterion': 'gini'}\n",
      "Best Score: 0.873731343284\n"
     ]
    }
   ],
   "source": [
    "print 'Best Parameters: {}'.format(cv_RFC.best_params_)\n",
    "print 'Best Score: {}'.format(cv_RFC.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting time: 0.784 sec\n",
      "Prediction time: 0.021 sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "best_RFC = RandomForestClassifier(n_estimators = 25, criterion = 'gini')\n",
    "model_RFC = best_RFC.fit(x1_train, y1_train)\n",
    "print \"Fitting time: {} sec\".format(round(time() - t0,3))\n",
    "\n",
    "t1 = time()\n",
    "y1_pred_RFC = model_RFC.predict(x1_test)\n",
    "print 'Prediction time: {} sec'.format(round(time() - t1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1431  250]\n",
      " [ 153 1466]]\n",
      "Percent Incorrect: 13.9109423542%\n"
     ]
    }
   ],
   "source": [
    "print confusion_matrix(y1_test, y1_pred_RFC)\n",
    "print 'Percent Incorrect: {}%'.format(float(250+153)*100/(1431+1466))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X2 and Y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 25, 50], 'criterion': ['gini', 'entropy']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFC = RandomForestClassifier()\n",
    "param_grid_RFC = { 'n_estimators' : [5,25,50],\n",
    "                  'criterion' : ['gini', 'entropy']}\n",
    "cv_RFC = GridSearchCV(estimator = RFC, param_grid=param_grid_RFC, cv=5)\n",
    "cv_RFC.fit(x2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 50, 'criterion': 'gini'}\n",
      "Best Score: 0.928955223881\n"
     ]
    }
   ],
   "source": [
    "print 'Best Parameters: {}'.format(cv_RFC.best_params_)\n",
    "print 'Best Score: {}'.format(cv_RFC.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting time: 0.519 sec\n",
      "Prediction time: 0.031 sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "best_RFC = RandomForestClassifier(n_estimators = 50, criterion = 'gini')\n",
    "model_RFC = best_RFC.fit(x2_train, y2_train)\n",
    "print \"Fitting time: {} sec\".format(round(time() - t0,3))\n",
    "\n",
    "t1 = time()\n",
    "y2_pred_RFC = model_RFC.predict(x2_test)\n",
    "print 'Prediction time: {} sec'.format(round(time() - t1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[777  73]\n",
      " [ 45 755]]\n",
      "Percent Incorrect: 7.70234986945%\n"
     ]
    }
   ],
   "source": [
    "print confusion_matrix(y2_test, y2_pred_RFC)\n",
    "print 'Percent Incorrect: {}%'.format(float(73+45)*100/(777+755))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X1 and Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'learning_rate': [0.01, 0.1, 1], 'max_depth': [1, 3, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBC = GradientBoostingClassifier()\n",
    "param_grid_GBC = {'learning_rate' : [.01, .1, 1],\n",
    "              'max_depth' : [1,3,5]}\n",
    "cv_GBC = GridSearchCV(estimator = GBC, param_grid=param_grid_GBC, cv=5)\n",
    "cv_GBC.fit(x1_train, y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 5}\n",
      "Best Score: 0.910447761194\n"
     ]
    }
   ],
   "source": [
    "print 'Best Parameters: {}'.format(cv_GBC.best_params_)\n",
    "print 'Best Score: {}'.format(cv_GBC.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting time: 3.26 sec\n",
      "Prediction time: 0.009 sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "best_GBC = GradientBoostingClassifier(learning_rate = .01,\n",
    "                                     max_depth = 5)\n",
    "model_GBC = best_GBC.fit(x1_train, y1_train)\n",
    "print \"Fitting time: {} sec\".format(round(time() - t0,3))\n",
    "\n",
    "t1 = time()\n",
    "y1_pred_GBC = model_GBC.predict(x1_test)\n",
    "print 'Prediction time: {} sec'.format(round(time() - t1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1451  230]\n",
      " [ 160 1459]]\n",
      "Percent Incorrect: 13.4020618557%\n"
     ]
    }
   ],
   "source": [
    "print confusion_matrix(y1_test, y1_pred_GBC)\n",
    "print 'Percent Incorrect: {}%'.format(float(230+160)*100/(1451+1459))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X2 and Y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'learning_rate': [0.01, 0.1, 1], 'max_depth': [1, 3, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBC = GradientBoostingClassifier()\n",
    "param_grid_GBC = {'learning_rate' : [.01, .1, 1],\n",
    "              'max_depth' : [1,3,5]}\n",
    "cv_GBC = GridSearchCV(estimator = GBC, param_grid=param_grid_GBC, cv=5)\n",
    "cv_GBC.fit(x2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 5}\n",
      "Best Score: 0.928358208955\n"
     ]
    }
   ],
   "source": [
    "print 'Best Parameters: {}'.format(cv_GBC.best_params_)\n",
    "print 'Best Score: {}'.format(cv_GBC.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting time: 1.179 sec\n",
      "Prediction time: 0.005 sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "best_GBC = GradientBoostingClassifier(learning_rate = .01,\n",
    "                                     max_depth = 5)\n",
    "model_GBC = best_GBC.fit(x2_train, y2_train)\n",
    "print \"Fitting time: {} sec\".format(round(time() - t0,3))\n",
    "\n",
    "t1 = time()\n",
    "y2_pred_GBC = model_GBC.predict(x2_test)\n",
    "print 'Prediction time: {} sec'.format(round(time() - t1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[731 119]\n",
      " [ 83 717]]\n",
      "Percent Incorrect: 13.9502762431%\n"
     ]
    }
   ],
   "source": [
    "print confusion_matrix(y2_test, y2_pred_GBC)\n",
    "print 'Percent Incorrect: {}%'.format(float(119+83)*100/(731+717))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Visualization using Bokeh (10 pts)\n",
    "\n",
    "In this problem, you'll build an interactive visualization. Bokeh is a Python interactive visualization library that targets modern web browsers for presentation. For more information on Bokeh, see http://bokeh.pydata.org/en/latest/. The problem statement is as follows:\n",
    "\n",
    "Using the \"nbasalariesfull.csv\" data set from HMK4, your goal is to build a Bokeh visualization which allows the user to explore how salary (on a log scale) varies with points per game (PSG) and age. You will create a visualization that allows the user to toggle the X axis of a scatter plot between PSG and age, with the y-axis always being log Salary. Also add the hover tool so that if the user hovers over a datapoint in the plot a window pops up that shows the player name, team, position, salary, and the current x variable (PSG or age) depending on the current tab.  Color each point according to a player's position and provide a legend for the colors. Add the ability to Zoom in/out.  Add slight horizontal jitter to a player's age.\n",
    "\n",
    "Hints: \n",
    "1. see: http://bokeh.pydata.org/en/latest/docs/user_guide/tools.html#basic-tooltips for hover and zoom tool examples.\n",
    "2. See: http://bokeh.pydata.org/en/latest/docs/reference/plotting.html. Look for the scatter API.\n",
    "3. See: http://bokeh.pydata.org/en/0.10.0/docs/user_guide/styling.html#labels. For labeling axes.\n",
    "4. See: https://bokeh.pydata.org/en/latest/docs/user_guide/categorical.html  for how to use jitter transform\n",
    "5. See: http://bokeh.pydata.org/en/latest/docs/gallery/iris.html for coloring points by category\n",
    "6. Use output_notebook() from Bokeh to output the plot to your notebook\n",
    "\n",
    "Include an image screenshot in addition to the visualization output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bokeh.models.widgets import Panel, Tabs\n",
    "from bokeh.models import CustomJS, ColumnDataSource, HoverTool, BoxZoomTool\n",
    "from bokeh.transform import jitter\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "\n",
    "data = pd.read_csv(\"nbasalariesfull.csv\")\n",
    "data[\"logsalary\"] = data.SALARY.apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Diabetes classification using support vector machines (4+3+3=10 pts) \n",
    "(a) Apply a linear SVM, using the scikit-SVM, for the Pima Indian Women diabetes detection problem on the dataset provided (details on dataset here  http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes) . Specify how you chose the slack cost/penalty (‘C’ parameter)for the model. Maintain all other parameters as default. Hint: http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html will make 10-fold cross-validation easier.\n",
    "The code to get the training/testing data is provided below.\n",
    "\n",
    "(b) Repeat (a) but using a Gaussian radial basis kernel.\n",
    "\n",
    "(c) Summarize the comparative performance (mean error rates) of the classifiers. What do you conclude? (be brief)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import grid_search\n",
    "\n",
    "data_train = pd.read_csv('diabetes_train-log.csv')\n",
    "data_test = pd.read_csv('diabetes_test-log.csv')\n",
    "cols = ['numpreg', 'plasmacon', 'bloodpress', 'skinfold', 'seruminsulin', 'BMI', 'pedigreefunction', 'age']\n",
    "\n",
    "xtrain = np.asmatrix(data_train[cols])\n",
    "ytrain = np.asarray(data_train['classvariable']).T\n",
    "\n",
    "xtest = np.asmatrix(data_test[cols])\n",
    "ytest = np.asarray(data_test['classvariable']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
