{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <p style=\"text-align: center;\">MIS382: Advanced Predictive Modeling</p>\n",
    "# <p style=\"text-align: center;\">Assignment 2</p>\n",
    "## <p style=\"text-align: center;\">Total points: 40</p>\n",
    "## <p style=\"text-align: center;\"> Timothy Lai (ttl353) and Meyappan Subbaiah (ms47296) </p>\n",
    "## <p style=\"text-align: center;\">Due: Wed, October 4th, submitted via Canvas by 11:59 pm</p>\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UTID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group.  \n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - Bias-variance Trandeoff (2pts)\n",
    "How does the choice of K in the K-nearest neighbor classifier reflect a bias-variance tradeoff?\n",
    "\n",
    "## Answer\n",
    "\n",
    "A low choice of K would reflect high variance and low bias. For example, a choice of K=1 would almost certainly overfit the data - the model would try to fit to each point and therefore we could also be incorporating a lot of noise from the data. Bias would be low since the average difference between estimated and true values is not skewed since every point is considered.\n",
    "\n",
    "Comparatively, a high choice of K would reflect low variance and high bias. In this case, we would likely be underfitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Data Exploration and Regression Analysis (4+3+4+4=15pts)\n",
    "\n",
    "Consider the dataset provided (russett_full_v2.csv) about agricultural inequality, industrial development and political instability in different countries. More information about it can be found [here](https://www.rdocumentation.org/packages/plspm/versions/0.4.9/topics/russett) though the data itself is slightly different than that referenced in the link.\n",
    "\n",
    "a) (4 points) Generate box-plots of the \"rent\" (% of farmers that rent all their land), \"inst\" (measure of political stability in the executive branch), \"ecks\" (number of violent internal war incidents ) and \"demo_score\" ( derived measure of the level of a country's democracy from 1945 to 61 ) and identify the cutoff values for outliers. \n",
    "\n",
    "Generate 3 scatterplots of \"rent\" against \"demo_score\", \"inst\" against \"demo_score\" and \"ecks\" against \"demo_score\" with the identified outliers colored differently than non-outliers in each; comment on how inclusion of the outliers would affect a predictive model for the \"demo_score\" response.  \n",
    "\n",
    "b) (3 points) Let us try to fit an MLR, using ordinary least squares, to this dataset with \"demo_score\" as the dependent variable using only the predictors 'rent','inst', and 'ecks' . \n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.30, random_state=22)   \n",
    "\n",
    "Report the RMSE obtained on both X_train and X_test. How much does this increase when you score your model on X_test?\n",
    "\n",
    "c) (4 points ) Try to predict ”demo_score” using a robust regression using Huber loss.  You can use the [sklearn package](  http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html).  Set regularization parameter alpha to 0.0 and all other parameters as default.\n",
    "Report RMSE obtained on both X_train and X_test.\n",
    "\n",
    "d) (4 points ) Compare and comment on the model fits obtained in (b) and (c) and plot the residual plots using all data for each model. How do outliers affect the relative performance of ordinary least squares regression, and robust regression with Huber loss in general? \n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>gini</th>\n",
       "      <th>farm</th>\n",
       "      <th>rent</th>\n",
       "      <th>gnpr</th>\n",
       "      <th>labo</th>\n",
       "      <th>inst</th>\n",
       "      <th>ecks</th>\n",
       "      <th>death</th>\n",
       "      <th>demo_class</th>\n",
       "      <th>demo_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>86.3</td>\n",
       "      <td>98.2</td>\n",
       "      <td>47.9</td>\n",
       "      <td>374</td>\n",
       "      <td>25</td>\n",
       "      <td>16.6</td>\n",
       "      <td>47.0</td>\n",
       "      <td>217</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australia</td>\n",
       "      <td>92.9</td>\n",
       "      <td>99.6</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1215</td>\n",
       "      <td>14</td>\n",
       "      <td>19.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austria</td>\n",
       "      <td>74.0</td>\n",
       "      <td>97.4</td>\n",
       "      <td>25.7</td>\n",
       "      <td>532</td>\n",
       "      <td>32</td>\n",
       "      <td>15.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>58.7</td>\n",
       "      <td>85.8</td>\n",
       "      <td>98.3</td>\n",
       "      <td>1015</td>\n",
       "      <td>10</td>\n",
       "      <td>23.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bolivia</td>\n",
       "      <td>93.8</td>\n",
       "      <td>97.7</td>\n",
       "      <td>35.0</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>18.3</td>\n",
       "      <td>43.0</td>\n",
       "      <td>663</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     country  gini  farm  rent  gnpr  labo  inst  ecks  death  demo_class  \\\n",
       "0  Argentina  86.3  98.2  47.9   374    25  16.6  47.0    217           2   \n",
       "1  Australia  92.9  99.6  30.0  1215    14  19.3   0.0      0           1   \n",
       "2    Austria  74.0  97.4  25.7   532    32  15.8   4.0      0           2   \n",
       "3    Belgium  58.7  85.8  98.3  1015    10  23.5   8.0      1           1   \n",
       "4    Bolivia  93.8  97.7  35.0    66    72  18.3  43.0    663           3   \n",
       "\n",
       "   demo_score  \n",
       "0          35  \n",
       "1          89  \n",
       "2          52  \n",
       "3          98  \n",
       "4          19  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "russett = pd.read_csv('russett_full_v2.csv')\n",
    "russett.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "boxpoints": "all",
         "fillcolor": "rgba(93, 164, 214, 0.5)",
         "jitter": 0.5,
         "line": {
          "width": 1
         },
         "marker": {
          "size": 2
         },
         "name": "rent",
         "type": "box",
         "whiskerwidth": 0.2,
         "y": [
          47.9,
          30,
          25.7,
          98.3,
          35,
          24.1,
          7.2,
          28.4,
          27.1,
          20.4,
          68.8,
          3.5,
          35.8,
          29.6,
          26.6,
          30.1,
          17.4,
          41,
          32,
          32.7,
          31.7,
          53,
          98,
          2.5,
          38.8,
          17.9,
          23.5,
          18.8,
          53.3,
          22.3,
          33.8,
          7.5,
          27.3,
          38.8,
          52.3,
          15,
          35,
          58.7,
          18.9,
          18.9,
          55,
          44.5,
          20.4,
          34.7,
          35.6,
          5.7,
          15
         ]
        },
        {
         "boxpoints": "all",
         "fillcolor": "rgba(255, 144, 14, 0.5)",
         "jitter": 0.5,
         "line": {
          "width": 1
         },
         "marker": {
          "size": 2
         },
         "name": "inst",
         "type": "box",
         "whiskerwidth": 0.2,
         "y": [
          16.6,
          19.3,
          15.8,
          23.5,
          18.3,
          18.5,
          19.3,
          17.2,
          17.6,
          17.6,
          16.6,
          22.6,
          14.3,
          18.1,
          18.8,
          18.1,
          18.6,
          19.3,
          17.9,
          18.8,
          16.6,
          0.1,
          19.2,
          22.2,
          18.5,
          18.7,
          17.8,
          20.8,
          21.6,
          20.8,
          15.8,
          20.8,
          18.6,
          17.6,
          17,
          11.5,
          13,
          0,
          16.5,
          16.5,
          0.1,
          21.6,
          20.8,
          22.6,
          17.9,
          0.1,
          0.1
         ]
        },
        {
         "boxpoints": "all",
         "fillcolor": "rgba(44, 160, 101, 0.5)",
         "jitter": 0.5,
         "line": {
          "width": 1
         },
         "marker": {
          "size": 2
         },
         "name": "ecks",
         "type": "box",
         "whiskerwidth": 0.2,
         "y": [
          47,
          0,
          4,
          8,
          43,
          29,
          12,
          11,
          27,
          19,
          100,
          0,
          6,
          21,
          25,
          9,
          4,
          26,
          25,
          9,
          25,
          83,
          14,
          9,
          41,
          12,
          8,
          0,
          2,
          0,
          16,
          1,
          19,
          13,
          15,
          19,
          40,
          12,
          0.1,
          0,
          3,
          12,
          12,
          1,
          16,
          4,
          9
         ]
        },
        {
         "boxpoints": "all",
         "fillcolor": "rgba(255, 65, 54, 0.5)",
         "jitter": 0.5,
         "line": {
          "width": 1
         },
         "marker": {
          "size": 2
         },
         "name": "demo_score",
         "type": "box",
         "whiskerwidth": 0.2,
         "y": [
          35,
          89,
          52,
          98,
          19,
          54,
          81,
          37,
          54,
          43,
          6,
          96,
          23,
          13,
          27,
          0,
          40,
          39,
          19,
          55,
          0,
          75,
          29,
          91,
          57,
          33,
          19,
          81,
          75,
          95,
          22,
          97,
          33,
          12,
          30,
          31,
          12,
          26,
          85,
          97,
          5,
          73,
          78,
          68,
          11,
          61,
          33
         ]
        }
       ],
       "layout": {
        "margin": {
         "b": 80,
         "l": 40,
         "r": 30,
         "t": 100
        },
        "paper_bgcolor": "rgb(243, 243, 243)",
        "plot_bgcolor": "rgb(243, 243, 243)",
        "showlegend": false,
        "title": "Box Plots for Various Russett Attributes",
        "yaxis": {
         "autorange": true,
         "dtick": 5,
         "gridcolor": "rgb(255, 255, 255)",
         "gridwidth": 1,
         "showgrid": true,
         "zeroline": true,
         "zerolinecolor": "rgb(255, 255, 255)",
         "zerolinewidth": 2
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_data = ['rent', 'inst', 'ecks', 'demo_score']\n",
    "\n",
    "y0 = russett['rent']\n",
    "y1 = russett['inst']\n",
    "y2 = russett['ecks']\n",
    "y3 = russett['demo_score']\n",
    "\n",
    "y_data = [y0,y1,y2,y3]\n",
    "\n",
    "colors = ['rgba(93, 164, 214, 0.5)', 'rgba(255, 144, 14, 0.5)', 'rgba(44, 160, 101, 0.5)', 'rgba(255, 65, 54, 0.5)']\n",
    "\n",
    "traces = []\n",
    "\n",
    "for xd, yd, cls in zip(x_data, y_data, colors):\n",
    "        traces.append(go.Box(\n",
    "            y=yd,\n",
    "            name=xd,\n",
    "            boxpoints='all',\n",
    "            jitter=0.5,\n",
    "            whiskerwidth=0.2,\n",
    "            fillcolor=cls,\n",
    "            marker=dict(\n",
    "                size=2,\n",
    "            ),\n",
    "            line=dict(width=1),\n",
    "        ))\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Box Plots for Various Russett Attributes',\n",
    "    yaxis=dict(\n",
    "        autorange=True,\n",
    "        showgrid=True,\n",
    "        zeroline=True,\n",
    "        dtick=5,\n",
    "        gridcolor='rgb(255, 255, 255)',\n",
    "        gridwidth=1,\n",
    "        zerolinecolor='rgb(255, 255, 255)',\n",
    "        zerolinewidth=2,\n",
    "    ),\n",
    "    margin=dict(\n",
    "        l=40,\n",
    "        r=30,\n",
    "        b=80,\n",
    "        t=100,\n",
    "    ),\n",
    "    paper_bgcolor='rgb(243, 243, 243)',\n",
    "    plot_bgcolor='rgb(243, 243, 243)',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "plotly.offline.iplot(fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 - Sampling (4+4=8pts)\n",
    "\n",
    "1. NBC has come up with an extreme TV show, and each of its viewers either likes or hates it. (no middle ground here; we are in a “black and white age”). NBC wants to estimate what fraction p of its audience like the show by “randomly” calling n viewers and tallying their responses so as to estimate the true value of p to a fractional accuracy of within ±ε%, with a confidence of (1 − α) × 100%. For α = 0.10, ε = 0.03 (i.e. your answer will be $\\hat{p}$ ± 0.03), what is the minimum value of n needed if true value (i) p = 0.5, (ii) p=0.25? \n",
    "\n",
    "2. Suppose for a certain value of p and choice of ε, you calculate that you will need 1000 samples for α = 0.02. You now decide to obtain a more accurate answer by either (i) reducing α to 0.01, keeping the same ε or by (ii) reducing ε by a factor of 2 from the original value, and increasing α to 0.05. In each case how many samples would you need now?\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 - Principal Component Analysis (10 points)\n",
    "\n",
    "Import the diabetes dataset as in Q1 and add the interaction variables.\n",
    "You should have 65 variables and one target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = PolynomialFeatures(2, include_bias=False).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to center and scale each feature as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "stdScaler = StandardScaler()\n",
    "X = stdScaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Perform PCA using the sklearn [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) package.  Create i) a scree plot depicting the proportion of variance and ii) a cumulative proportion of variance explained by the principal components of the data (X matrix).  Refer to Figure 10.4 of JW for an example.  You may use the output attribute *explained variance ratio*. (3pts)\n",
    "\n",
    "(b) How many principal components (N1, N2, N3) are required to explain cumulative variance of 30%, 60%, and 90%, respectively? (3pts)\n",
    "\n",
    "(c) Fit an ordinary least squares linear regression using N1, N2, and N3 number of principal components, respectively. (This is called Principal Components Regression) Use entire dataset, e.g. 442 rows. Evaluate the models using mean squared error (MSE). (4pts)\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.16576031  0.09110488]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "print(pca.explained_variance_ratio_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFNCAYAAAD7De1wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREQSTEEuIiKSYApyERGRBPv/g/obP2/IJjUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116e12090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## NOTE (Tim) - I found the below code as a sample to generate scree plots. We need to play with this.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Make a random array and then make it positive-definite\n",
    "num_vars = 6\n",
    "num_obs = 9\n",
    "A = np.random.randn(num_obs, num_vars)\n",
    "A = np.asmatrix(A.T) * np.asmatrix(A)\n",
    "U, S, V = np.linalg.svd(A) \n",
    "eigvals = S**2 / np.cumsum(S)[-1]\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "sing_vals = np.arange(num_vars) + 1\n",
    "plt.plot(sing_vals, eigvals, 'ro-', linewidth=2)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Eigenvalue')\n",
    "\n",
    "leg = plt.legend(['Eigenvalues'], loc='best', borderpad=0.3, \n",
    "                 shadow=False, prop=matplotlib.font_manager.FontProperties(size='small'),\n",
    "                 markerscale=0.4)\n",
    "leg.get_frame().set_alpha(0.4)\n",
    "leg.draggable(state=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 -  Feature Selection ( 5 points )\n",
    "\n",
    "Explain what you understand by the two wrapper methods for feature selection (forward and backward selection) (no more than 1 paragraph). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Both wrapper methods are used for variable selection. In forward selection, we begin with the null model and fit `p` simple linear regressions and add to the null model until we have the lowest RSS, which measures the amount of variability that is left unexplained after performing the regression. Comparatively in backward selection, all variables are included in the model and we remove the variable with the largest p-value in an iterative process until a stopping rule is reached. Forward selection can always be used, but generally is considered more \"greedy\", since we may include variables early that later become redundant."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
